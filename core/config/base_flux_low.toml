## utak atik

no_metadata = true
ae = "/app/flux/ae.safetensors"                                                                                                                                      
apply_t5_attn_mask = true                                                                                                                                                      
bucket_no_upscale = true                                                                                                                                                       
bucket_reso_steps = 64                                                                                                                                                         
cache_latents = true                                                                                                                                                           
cache_latents_to_disk = true                                                                                                                                                   
caption_extension = ".txt"                                                                                                                                                     
clip_l = "/app/flux/clip_l.safetensors"                                                                                                                              
discrete_flow_shift = 3.1582                                                                                                                                                   
dynamo_backend = "no"                                                                                                                                                          
full_bf16 = true                                                                                                                                                               
gradient_accumulation_steps = 16                                                                                                                                                
gradient_checkpointing = true                                                                                                                                                  
guidance_scale = 80.0                                                                                                                                                           
highvram = true                                                                                                                                                                
huber_c = 0.05                                                                                                                                                                  
huber_scale = 1                                                                                                                                                                
huber_schedule = "exponential"
huggingface_path_in_repo = "checkpoint"
huggingface_repo_id = ""
huggingface_repo_type = "model"
huggingface_repo_visibility = "public"
huggingface_token = ""                                                                                                                                                         
learning_rate = 0.001
loss_type = "huber"
lr_scheduler = "cosine"                                                                                                                                                      
lr_scheduler_args = []                                                                                                                                                         
lr_scheduler_num_cycles = 1                                                                                                                                                    
lr_scheduler_power = 1                                                                                                                                                         
max_bucket_reso = 2048                                                                                                                                                         
max_data_loader_n_workers = 0                                                                                                                                                  
max_timestep = 1000                                                                                                                                                            
max_train_steps = 1
mem_eff_save = true                                                                                                                                                            
min_bucket_reso = 256                                                                                                                                                          
mixed_precision = "bf16"                                                                                                                                                       
model_prediction_type = "raw"                                                                                                                                                  
network_alpha = 64                                                                                                                                                            
network_args = [ "train_double_block_indices=all", "train_single_block_indices=all", "train_t5xxl=True",]                                                                      
network_dim = 64                                                                                                                                                              
network_module = "networks.lora_flux"                                                                                                                                          
noise_offset_type = "Original"                                                                                                                                                 
optimizer_args = ["betas=(0.9,0.999)", "weight_decay=0.0"]                                                             
optimizer_type = "AdamW8bit"                                                                                                                                                   
output_dir = "/app/outputs"                                                                                                                                          
output_name = "last"                                                                                                                                                 
pretrained_model_name_or_path = "/app/flux/unet.safetensors"                                                                                                                  
prior_loss_weight = 1                                                                                                                                                          
resolution = "512,512"                                                                                                                                                       
sample_prompts = ""                                                                                                                    
sample_sampler = "euler_a"                                                                                                                                                     
save_every_n_epochs = 10                                                                                                                                                       
save_model_as = "safetensors"                                                                                                                                                  
save_precision = "float"                                                                                                                                                       
seed = 1                                                                                                                                                                       
t5xxl = "/app/flux/t5xxl_fp16.safetensors"                                                                                                                           
t5xxl_max_token_length = 512                                                                                                                                                   
text_encoder_lr = [0.0, 0.0]
timestep_sampling = "sigmoid"                                                                                                                                                  
train_batch_size = 1                                                                                                                                                           
train_data_dir = ""                                                                                                                               
unet_lr = 0.001                                                                                                                                                             
vae_batch_size = 4                                                                                                                                                             
wandb_run_name = "last"                                                                                                                                              
xformers = true         
